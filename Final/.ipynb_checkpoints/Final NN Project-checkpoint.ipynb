{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Exploration of TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Throughout my exploration of machine learning in the past, I have always been interested in Nueral Networks.  Nueral Networks, in my opinon, appear to be the pinical of ML, and I have been excited by their capabilities since the day I first learned about them.\n",
    "\n",
    "When I heard about TensorFlow, the Nueral Network python library from Google, I was extatic about its posibilities.  Due to this prior interest, I selected an exploration of TensorFlow as my final project for the Big Data class.\n",
    "\n",
    "While TensorFlow can prove to be difficult to pick up, with a little support from the internet and those around you, it has real potential in implementing nueral networks faster than ever before.\n",
    "\n",
    "For this exploration, I chose a simple problem for the Neural network to solve.  Data containing the information of 5 different playing cards would be given as input, and the type of hand would be requested as an output.  This problem is one of classification, and in order to most effectivly solve it while roaming in a new field, I based my nueral networks off of the classification tutorials at www.tensorflow.org.  Along with help from Ms. Anderson and my fellow student Malcolm Volk, I created two nueral networks to attempt and solve this proposed problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network 1\n",
    "\n",
    "A simple Nueral Network with zero hidden layers.\n",
    "\n",
    "I found a dataset from UC: Irvine that contained labeled training and testing data for this problem of card hand classisification.  This data is used throughout this lab and is cited below.\n",
    "\n",
    "This network functions without any hidden layers and attempts to classify each hand with the softmax classification model alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "#import all data\n",
    "#import training data\n",
    "f = open('poker-hand-training-true.data.txt', 'r')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "training_data, training_label = [],[]\n",
    "\n",
    "for row in csv_f:\n",
    "\t#create a list for this row's data\n",
    "\tthis_data = []\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tthis_data.append(int(row[i]))\n",
    "\ttraining_data.append(this_data)\n",
    "\t#get this row's label. A list is used in order to keep dimensions the same throughout\n",
    "\tthis_label = []\n",
    "\tthis_label.append(int(row[len(row)-1]))\n",
    "\t#this_label.append(float(row[len(row)-1])/9)\n",
    "\t#add the label to the full label list\n",
    "\thot_vector = np.eye(10)[int(row[len(row)-1])]\n",
    "\ttraining_label.append(hot_vector)\n",
    "\n",
    "#import testing data\n",
    "\n",
    "f = open('poker-hand-testing.data.txt', 'r')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "testing_data, testing_label = [],[]\n",
    "\n",
    "for row in csv_f:\n",
    "\t#create a list for this row's data\n",
    "\tthis_data = []\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tthis_data.append(int(row[i]))\n",
    "\ttesting_data.append(this_data)\n",
    "\t#get this row's label. A list is used in order to keep dimensions the same throughout\n",
    "\tthis_label = []\n",
    "\tthis_label.append(int(row[len(row)-1]))\n",
    "\t\n",
    "\t#add the label to the full label list\n",
    "\thot_vector = np.eye(10)[int(row[len(row)-1])]\n",
    "\ttesting_label.append(hot_vector)\n",
    "\n",
    "#function to get a batch of training data\n",
    "def getTrainingBatch(num):\n",
    "\tdata = []\n",
    "\tlabels = []\n",
    "\tfor i in range(num):\n",
    "\t\tindex = int(random.random() * len(training_data))\n",
    "\t\tdata.append(training_data[index])\n",
    "\t\tlabels.append(training_label[index])\n",
    "\treturn np.asarray(data),np.asarray(labels)\n",
    "\n",
    "\n",
    "n_inputs = 10\n",
    "n_features = 10\n",
    "n_classes = 10\n",
    "\n",
    "#initilization of the regression\n",
    "x = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "#weights\n",
    "W = tf.Variable(tf.zeros([n_inputs, n_classes]))\n",
    "#bias'\n",
    "b = tf.Variable(tf.zeros([n_classes]))\n",
    "\n",
    "#implement the softmax regression model \n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "\n",
    "#training section\n",
    "#implement our cost function).  This defines how the model should be trained\n",
    "y_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "cost = tf.reduce_mean(tf.abs(tf.sub(y_,y)))\n",
    "\n",
    "#train the model\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "#launch the model\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "#train 1000 times\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = getTrainingBatch(300)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "\n",
    "#confirm the accuracy of the model\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: np.asarray(testing_data), y_: np.asarray(testing_label)}))\n",
    "\n",
    "#this is testing data for a hand with the identifier 9\n",
    "print(sess.run(y, feed_dict={x:np.reshape(np.asarray([3,12,3,11,3,13,3,10,3,1]), (1,10))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy w/ Softmax: 0.501209\n",
    "\n",
    "Classify a Royal Flush (Hot vectors):\n",
    "[[ 0.9332422   0.0380508   0.00398424  0.00368423  0.00352931  0.00351424\n",
    "   0.00350735  0.00349597  0.0034956   0.00349621]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After expeirimenting with various training algorithms and between sigmoid and softmax models, I found that the configuration above of Gradient Descent Optimization and Softmax regression gave the most accurate results.  While these were the most accurate, at around 50% accuracy, this model does not function extremely well.\n",
    "\n",
    "One hypothesis I had for this phenomonon was the distribution of training data.  Playing card hands are not evenly distributed, and therefore neither is the training data.  Without hidden layers to normalize the network, as seen by testing this dataset with a test row, the predictions are distributed towards the lower hands, identified by 0 and 1.  Since these are very promininet within the dataset, the accuracy of 50% could be from only these hands being classified correctly.  Data that supports this is the attempted classification of a Royal Flush, idenfified by a 9, which our networks classifies as a 0, with a high probability of a 1.\n",
    "\n",
    "By adding hidden layers we can attempt to improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 2\n",
    "\n",
    "This is an improved version of the network created above.  This network was created with the guideance of Malcolm Volk and the Deep MINST for Experts tutorial found here.\n",
    "https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html\n",
    "\n",
    "This network includes 4 hidden layers two of which use ReLU, a common activation layer function, and two of which use sigmoid regression.  Sigmoid regression was chosen for this problem because it can returns values between 0 and 1.  Our dataset returns 10 possible classes, we can use sigmoid to return a decimal and later convert it to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "#import all data\n",
    "#import training data\n",
    "f = open('poker-hand-training-true.data.txt', 'r')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "training_data, training_label = [],[]\n",
    "\n",
    "for row in csv_f:\n",
    "\t#create a list for this row's data\n",
    "\tthis_data = []\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tthis_data.append(int(row[i]))\n",
    "\ttraining_data.append(this_data)\n",
    "\t#get this row's label. A list is used in order to keep dimensions the same throughout\n",
    "\tthis_label = []\n",
    "\tthis_label.append(float(row[len(row)-1])/9)\n",
    "\t#add the label to the full label list\n",
    "\t#hot_vector = np.eye(10)[int(row[len(row)-1])]\n",
    "\ttraining_label.append(this_label)\n",
    "\n",
    "#import testing data\n",
    "\n",
    "f = open('poker-hand-testing.data.txt', 'r')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "testing_data, testing_label = [],[]\n",
    "\n",
    "for row in csv_f:\n",
    "\t#create a list for this row's data\n",
    "\tthis_data = []\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tthis_data.append(int(row[i]))\n",
    "\ttesting_data.append(this_data)\n",
    "\t#get this row's label. A list is used in order to keep dimensions the same throughout\n",
    "\tthis_label = []\n",
    "\tthis_label.append(float(row[len(row)-1])/9)\n",
    "\t\n",
    "\t#add the label to the full label list\n",
    "\ttesting_label.append(this_label)\n",
    "\n",
    "#function to get a batch of training data\n",
    "def getTrainingBatch(num):\n",
    "\tdata = []\n",
    "\tlabels = []\n",
    "\tfor i in range(num):\n",
    "\t\tindex = int(random.random() * len(training_data))\n",
    "\t\tdata.append(training_data[index])\n",
    "\t\tlabels.append(training_label[index])\n",
    "\treturn np.asarray(data),np.asarray(labels)\n",
    "\n",
    "#these functions create more effective weights + bias'\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "#ensure that there will be no 'dead' nuerons by starting with a slightly positive initial bias\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "n_features=10\n",
    "n_features_2=10\n",
    "n_features_3=10\n",
    "n_features_4=10\n",
    "n_input=10\n",
    "n_classes=1\n",
    "n_layers=5\n",
    "\n",
    "#inputs\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "#weights\n",
    "weight_1 = weight_variable([n_input, n_features])\n",
    "weight_2 = weight_variable([n_features, n_features_2])\n",
    "weight_3 = weight_variable([n_features_2, n_features_3])\n",
    "weight_4 = weight_variable([n_features_3, n_features_4])\n",
    "weight_5 = weight_variable([n_features_4, n_classes])\n",
    "\n",
    "#bias\n",
    "bias_1 = bias_variable([n_features])\n",
    "bias_2 = bias_variable([n_features_2])\n",
    "bias_3 = bias_variable([n_features_3])\n",
    "bias_4 = bias_variable([n_features_4])\n",
    "bias_5 = bias_variable([n_classes])\n",
    "\n",
    "#model\n",
    "def model(x):\n",
    "    hidden_1 = tf.nn.relu(tf.matmul(x, weight_1) + bias_1)\n",
    "    hidden_2 = tf.nn.relu(tf.matmul(hidden_1, weight_2) + bias_2)\n",
    "    hidden_3 = tf.nn.sigmoid(tf.matmul(hidden_2, weight_3) + bias_3)\n",
    "    hidden_4 = tf.nn.sigmoid(tf.matmul(hidden_3, weight_4) + bias_4)\n",
    "    output = 10 * tf.nn.sigmoid(tf.matmul(hidden_4, weight_5) + bias_5)\n",
    "    return output\n",
    "\n",
    "cost = tf.reduce_mean(tf.abs(tf.sub(model(x),y_)))\n",
    "train_step = tf.train.GradientDescentOptimizer(.001).minimize(cost)\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "#train 1000 times\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = getTrainingBatch(300)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "\n",
    "accuracy = 1-(tf.reduce_mean(tf.abs(tf.sub(tf.round(model(x)),y_))))/10\n",
    "print(sess.run(accuracy, feed_dict={x: np.asarray(testing_data), y_: testing_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.99318\n",
    "\n",
    "With this new model we get a much higher accuracy of 99.318%.  This is a huge improvement over the ~50% given by the original network, all from adding a few hidden layers.  There is s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
